Step 2: Install Dependencies
In the Replit shell, run:

bash
pip install torch torchvision torchaudio librosa opencv-python transformers
Step 3: Training Code (Paste This in main.py)
This is a Replit-optimized version of your code, ready to train and save the model:

python
import os
import cv2
import torch
import librosa
import numpy as np
from torch import nn
from torchvision import transforms
from torch.utils.data import Dataset, DataLoader

# CONFIGURATION
DATA_DIR = 'data'
VIDEO_DIR = os.path.join(DATA_DIR, 'video')
AUDIO_DIR = os.path.join(DATA_DIR, 'audio')
IMG_SIZE = 112
BATCH_SIZE = 4  # Lowered for Replit memory
EPOCHS = 3      # Adjust as needed
LEARNING_RATE = 0.0005
FRAMES_PER_CLIP = 16
MODEL_PATH = 'scammer_model_v1.pth'

class VideoAudioDataset(Dataset):
    def __init__(self, root_dir, transform=None):
        self.samples = []
        self.transform = transform
        for label in ['scammer', 'genuine']:
            for video_file in os.listdir(os.path.join(VIDEO_DIR, label)):
                if video_file.endswith('.mp4'):
                    base_name = os.path.splitext(video_file)[0]
                    audio_file = os.path.join(AUDIO_DIR, label, f"{base_name}.wav")
                    if os.path.exists(audio_file):
                        self.samples.append((
                            os.path.join(VIDEO_DIR, label, video_file),
                            audio_file,
                            1 if label == 'scammer' else 0
                        ))

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        video_path, audio_path, label = self.samples[idx]
        frames = self._load_video_frames(video_path)
        audio_features = self._extract_audio_features(audio_path)
        return {
            'frames': frames,
            'audio': audio_features,
            'label': torch.tensor(label, dtype=torch.long)
        }

    def _load_video_frames(self, video_path):
        cap = cv2.VideoCapture(video_path)
        frames = []
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        for i in range(FRAMES_PER_CLIP):
            frame_idx = int(i * (total_frames / FRAMES_PER_CLIP))
            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)
            ret, frame = cap.read()
            if ret:
                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                if self.transform:
                    frame = self.transform(frame)
                frames.append(frame)
        cap.release()
        while len(frames) < FRAMES_PER_CLIP:
            frames.append(frames[-1])
        return torch.stack(frames)

    def _extract_audio_features(self, audio_path):
        y, sr = librosa.load(audio_path, sr=None)
        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)
        chroma = librosa.feature.chroma_stft(y=y, sr=sr)
        spectral_contrast = librosa.feature.spectral_contrast(y=y, sr=sr)
        audio_features = np.vstack([mfcc, chroma, spectral_contrast])
        audio_features = (audio_features - audio_features.mean()) / (audio_features.std() + 1e-8)
        return torch.from_numpy(audio_features).float()

class ScammerDetector(nn.Module):
    def __init__(self, num_classes=2):
        super().__init__()
        self.visual_net = nn.Sequential(
            nn.Conv3d(3, 32, kernel_size=(3, 3, 3), padding=(1, 1, 1)),
            nn.BatchNorm3d(32), nn.ReLU(), nn.MaxPool3d((1, 2, 2)),
            nn.Conv3d(32, 64, kernel_size=(3, 3, 3), padding=(1, 1, 1)),
            nn.BatchNorm3d(64), nn.ReLU(), nn.MaxPool3d((1, 2, 2)),
            nn.Conv3d(64, 128, kernel_size=(3, 3, 3), padding=(1, 1, 1)),
            nn.BatchNorm3d(128), nn.ReLU(), nn.MaxPool3d((1, 2, 2))
        )
        self.audio_net = nn.Sequential(
            nn.Conv1d(28, 32, kernel_size=3, padding=1), nn.BatchNorm1d(32), nn.ReLU(), nn.MaxPool1d(2),
            nn.Conv1d(32, 64, kernel_size=3, padding=1), nn.BatchNorm1d(64), nn.ReLU(), nn.MaxPool1d(2),
            nn.Conv1d(64, 128, kernel_size=3, padding=1), nn.BatchNorm1d(128), nn.ReLU(), nn.MaxPool1d(2)
        )
        self.attention = nn.Sequential(
            nn.Linear(256, 64), nn.ReLU(), nn.Linear(64, 1), nn.Sigmoid()
        )
        self.classifier = nn.Sequential(
            nn.Linear(256, 128), nn.ReLU(), nn.Dropout(0.5), nn.Linear(128, num_classes)
        )

    def forward(self, x):
        video_frames = x['frames']
        audio_features = x['audio']
        visual_out = self.visual_net(video_frames.permute(1, 0, 2, 3).unsqueeze(0))
        visual_out = visual_out.mean(dim=[2, 3, 4])
        audio_out = self.audio_net(audio_features.unsqueeze(0))
        audio_out = audio_out.mean(dim=2)
        combined = torch.cat([visual_out, audio_out], dim=1)
        attention_weights = self.attention(combined)
        weighted_combined = combined * attention_weights
        return self.classifier(weighted_combined)

def train_model():
    transform = transforms.Compose([
        transforms.ToPILImage(),
        transforms.Resize((IMG_SIZE, IMG_SIZE)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ])
    dataset = VideoAudioDataset(DATA_DIR, transform=transform)
    train_size = int(0.8 * len(dataset))
    val_size = len(dataset) - train_size
    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])
    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)
    model = ScammerDetector()
    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)
    for epoch in range(EPOCHS):
        model.train()
        running_loss = 0.0
        correct = 0
        total = 0
        for batch in train_loader:
            optimizer.zero_grad()
            outputs = model(batch)
            loss = criterion(outputs, batch['label'])
            loss.backward()
            optimizer.step()
            running_loss += loss.item()
            _, predicted = torch.max(outputs.data, 1)
            total += batch['label'].size(0)
            correct += (predicted == batch['label']).sum().item()
        train_acc = 100 * correct / total
        val_acc = evaluate(model, val_loader)
        print(f"Epoch {epoch+1}/{EPOCHS} | Loss: {running_loss/len(train_loader):.4f} | "
              f"Train Acc: {train_acc:.2f}% | Val Acc: {val_acc:.2f}%")
    torch.save(model.state_dict(), MODEL_PATH)
    print(f"Model saved to {MODEL_PATH}")

def evaluate(model, loader):
    model.eval()
    correct = 0
    total = 0
    with torch.no_grad():
        for batch in loader:
            outputs = model(batch)
            _, predicted = torch.max(outputs.data, 1)
            total += batch['label'].size(0)
            correct += (predicted == batch['label']).sum().item()
    return 100 * correct / total

if __name__ == "__main__":
    print("Starting training...")
    train_model()
Step 4: Run the Training
In the Replit console, run:

bash
python main.py
This will train your model and save it as scammer_model_v1.pth.

Step 5: How to Use the Trained Model
To use the trained model for inference in a new script (e.g., inference.py):

python
import torch
from your_model_file import ScammerDetector  # Use the same class definition as above

model = ScammerDetector()
model.load_state_dict(torch.load('scammer_model_v1.pth'))
model.eval()

# Prepare your input as in the training script, then:
# output = model({'frames': ..., 'audio': ...})
Prompt for Replit
"Train the multimodal scammer detection model on my video/audio dataset using the provided code, then use the saved model (scammer_model_v1.pth) for inference as shown."

This workflow is fully Replit-ready and will let you train and use your scammer detection model on your own data.